{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "941d61ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /Users/csarat/git/allstarscricket/allstars_dbt\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "import requests\n",
    "import click\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import psycopg2\n",
    "import polars as pl\n",
    "from polars.exceptions import ComputeError\n",
    "\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).parent.parent.parent.resolve()\n",
    "except NameError:\n",
    "    # Fallback for interactive sessions (no __file__)\n",
    "    BASE_DIR = Path(os.getcwd()).parent.parent.resolve()\n",
    "LOG_DIR = BASE_DIR / \"logs\"\n",
    "\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "\n",
    "def setup_logging(\n",
    "    log_dir: str = LOG_DIR,\n",
    "    logger_name: str = \"dash_extract\",\n",
    "    when: str = \"midnight\",\n",
    "    interval: int = 1,\n",
    "    backup_count: int = 7,\n",
    "    level: int = logging.INFO,\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup logging with both console and time-based rotating file handlers.\n",
    "\n",
    "    Args:\n",
    "        log_dir (str): directory where logs are stored\n",
    "        log_file (str): base log file name\n",
    "        when (str): rotation interval type (e.g., 'S','M','H','D','midnight','W0'-'W6')\n",
    "        interval (int): number of intervals between rotations\n",
    "        backup_count (int): how many old logs to keep\n",
    "        level (int): logging level\n",
    "    \"\"\"\n",
    "    Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "    log_file = f\"{logger_name}.log\"\n",
    "    log_path = os.path.join(log_dir, log_file)\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    # Rotating file handler\n",
    "    file_handler = TimedRotatingFileHandler(\n",
    "        log_path,\n",
    "        when=when,\n",
    "        interval=interval,\n",
    "        backupCount=backup_count,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Root logger config\n",
    "    logging.basicConfig(level=level, handlers=[file_handler, console_handler])\n",
    "\n",
    "    return logging.getLogger(logger_name)\n",
    "\n",
    "\n",
    "def get_credentials():\n",
    "    with open(BASE_DIR / \"env.json\", \"r\") as file:\n",
    "        creds = json.load(file)\n",
    "    return creds\n",
    "\n",
    "\n",
    "credentials = get_credentials()\n",
    "\n",
    "DAYSMART_CLIENT_ID = credentials[\"DAYSMART_CLIENT_ID\"]\n",
    "DAYSMART_CLIENT_SECRET = credentials[\"DAYSMART_CLIENT_SECRET\"]\n",
    "DAYSMART_API_GRANT_TYPE = \"client_credentials\"\n",
    "POSTGRES_DB = credentials[\"POSTGRES_DB\"]\n",
    "POSTGRES_USER = credentials[\"POSTGRES_USER\"]\n",
    "POSTGRES_PASSWORD = credentials[\"POSTGRES_PASSWORD\"]\n",
    "S3_BUCKET = credentials[\"S3_BUCKET\"]\n",
    "BASE_URL = \"https://api.dashplatform.com\"\n",
    "LOG = setup_logging()\n",
    "\n",
    "\n",
    "def get_bearer_token():\n",
    "    url = BASE_URL + \"/v1/auth/token\"\n",
    "    headers = {\"Content-Type\": \"application/vnd.api+json\"}\n",
    "    payload = {\n",
    "        \"grant_type\": DAYSMART_API_GRANT_TYPE,\n",
    "        \"client_id\": DAYSMART_CLIENT_ID,\n",
    "        \"client_secret\": DAYSMART_CLIENT_SECRET,\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"access_token\")\n",
    "    else:\n",
    "        raise Exception(response.json())\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Fetcher\n",
    "# -------------------------\n",
    "def get_index_data(end_point, bearer_token):\n",
    "    url = BASE_URL + \"/v1/\" + end_point\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/vnd.api+json\",\n",
    "        \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "    }\n",
    "    params = {\"company\": \"allstars\", \"sort\": \"id\"}\n",
    "    all_data = []\n",
    "    next_url = url\n",
    "    while next_url:\n",
    "        response = requests.get(next_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # Append the current page's data (assuming it's under a 'data' key)\n",
    "            if \"data\" in data:\n",
    "                all_data.extend(data[\"data\"])\n",
    "\n",
    "            # Update the next_url for the next iteration\n",
    "            next_url = data.get(\"links\", {}).get(\"next\")\n",
    "        else:\n",
    "            raise Exception(response.json())\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# JSON → Polars\n",
    "# -------------------------\n",
    "def records_to_polars(records: list[dict]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Lifts JSON:API objects (id/type + attributes.*).\n",
    "    Builds a Polars DF with full-length schema inference.\n",
    "    If mixed types are detected across rows, falls back to string-cast.\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return pl.DataFrame([])\n",
    "\n",
    "    def lift_one(r: dict):\n",
    "        if isinstance(r, dict) and \"attributes\" in r:\n",
    "            base = {\n",
    "                k: v for k, v in r.items() if k not in (\"attributes\", \"relationships\")\n",
    "            }\n",
    "            base.update(r.get(\"attributes\", {}))\n",
    "            return base\n",
    "        return r\n",
    "\n",
    "    lifted = [lift_one(r) for r in records]\n",
    "\n",
    "    # First attempt: infer using full data scan\n",
    "    try:\n",
    "        df = pl.from_dicts(lifted, infer_schema_length=None)  # scan all rows\n",
    "    except ComputeError:\n",
    "        # Fallback: cast every value to string (JSON for nested), preserving None\n",
    "        def to_str(v):\n",
    "            if v is None:\n",
    "                return None\n",
    "            # stringify nested structures to preserve info for CSV/SQL\n",
    "            if isinstance(v, (dict, list)):\n",
    "                return json.dumps(v, separators=(\",\", \":\"), default=str)\n",
    "            return str(v)\n",
    "\n",
    "        lifted_str = [{k: to_str(v) for k, v in row.items()} for row in lifted]\n",
    "        df = pl.from_dicts(lifted_str, infer_schema_length=None)\n",
    "\n",
    "    # # Unnest any 1-level struct columns if they exist (after first path)\n",
    "    # struct_cols = [\n",
    "    #     c for c, dt in zip(df.columns, df.dtypes) if isinstance(dt, pl.Struct)\n",
    "    # ]\n",
    "    # for c in struct_cols:\n",
    "    #     df = df.unnest(c)\n",
    "\n",
    "    # Put id first if present\n",
    "    if \"id\" in df.columns:\n",
    "        df = df.select([\"id\", *[c for c in df.columns if c != \"id\"]])\n",
    "\n",
    "    now = datetime.now()  # timezone-aware UTC timestamp\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.lit(now).alias(\"inserted_dt\"),\n",
    "            pl.lit(now).alias(\"updated_dt\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# S3 Writers\n",
    "# -------------------------\n",
    "def write_jsonl_to_s3(records: list[dict], bucket: str, key: str, s3_client=None):\n",
    "    \"\"\"\n",
    "    Writes list-of-dicts as JSONL.gz to s3://bucket/key.\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    with gzip.GzipFile(fileobj=buf, mode=\"wb\") as gz:\n",
    "        for r in records:\n",
    "            line = (json.dumps(r, separators=(\",\", \":\"), default=str) + \"\\n\").encode(\n",
    "                \"utf-8\"\n",
    "            )\n",
    "            gz.write(line)\n",
    "    body = buf.getvalue()\n",
    "\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=body,\n",
    "            ContentType=\"application/json\",\n",
    "            ContentEncoding=\"gzip\",\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"S3 JSONL upload failed: {e}\")\n",
    "\n",
    "\n",
    "def write_csv_to_s3_polars(\n",
    "    df: pl.DataFrame,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    compress: bool = False,\n",
    "    include_header: bool = True,\n",
    "    s3_client=None,\n",
    "    **to_csv_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Write polars DataFrame to s3://bucket/key as CSV (optionally .gz).\n",
    "    to_csv_kwargs → passed to Polars write_csv (e.g., separator=',', quote='\\\"').\n",
    "    \"\"\"\n",
    "    if df.height == 0:\n",
    "        return\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "    # Polars write_csv expects a text buffer; wrap a BytesIO.\n",
    "    raw_buf = io.BytesIO()\n",
    "    text_buf = io.TextIOWrapper(raw_buf, encoding=\"utf-8\", newline=\"\")\n",
    "    df.write_csv(text_buf, include_header=include_header, **to_csv_kwargs)\n",
    "    text_buf.flush()\n",
    "    raw_bytes = raw_buf.getvalue()\n",
    "\n",
    "    extra = {\"ContentType\": \"text/csv; charset=utf-8\"}\n",
    "    if compress:\n",
    "        gz = io.BytesIO()\n",
    "        with gzip.GzipFile(fileobj=gz, mode=\"wb\") as z:\n",
    "            z.write(raw_bytes)\n",
    "        body = gz.getvalue()\n",
    "        extra[\"ContentEncoding\"] = \"gzip\"\n",
    "    else:\n",
    "        body = raw_bytes\n",
    "\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=bucket, Key=key, Body=body, **extra)\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"S3 CSV upload failed: {e}\")\n",
    "\n",
    "\n",
    "def write_parquet_to_s3_polars(\n",
    "    df: pl.DataFrame,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    s3_client=None,\n",
    "    **to_parquet_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Write polars DataFrame to s3://bucket/key as Parquet.\n",
    "    to_parquet_kwargs → passed to Polars write_parquet (e.g., compression='snappy').\n",
    "    \"\"\"\n",
    "    if df.height == 0:\n",
    "        return\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "    # Write to bytes buffer\n",
    "    buf = io.BytesIO()\n",
    "    df.write_parquet(buf, **to_parquet_kwargs)\n",
    "    body = buf.getvalue()\n",
    "\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket, \n",
    "            Key=key, \n",
    "            Body=body,\n",
    "            ContentType=\"application/octet-stream\"\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"S3 Parquet upload failed: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# Postgres Loader (TRUNCATE + INSERT)\n",
    "# -------------------------\n",
    "def load_polars_df_to_postgres(\n",
    "    df: pl.DataFrame,\n",
    "    table: str,\n",
    "    conn_str: str,\n",
    "    schema: str = \"public\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Replaces target table: TRUNCATE then bulk INSERT via COPY.\n",
    "    Creates table if absent (all columns TEXT for simplicity).\n",
    "    \"\"\"\n",
    "    if df.height == 0:\n",
    "        LOG.info(\"No rows; skipping Postgres replace.\")\n",
    "        return\n",
    "\n",
    "    cols = df.columns\n",
    "    collist_sql = \", \".join([f'\"{c}\"' for c in cols])\n",
    "\n",
    "    conn = psycopg2.connect(conn_str)\n",
    "    conn.autocommit = False\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Create table if missing (all TEXT columns)\n",
    "            create_cols_sql = \", \".join([f'\"{c}\" text' for c in cols])\n",
    "\n",
    "            # Create schema\n",
    "            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n",
    "            # DROP target\n",
    "            cur.execute(f'DROP TABLE IF EXISTS \"{schema}\".\"{table}\";')\n",
    "            cur.execute(\n",
    "                f\"\"\"\n",
    "                DO $$\n",
    "                BEGIN\n",
    "                    IF NOT EXISTS (\n",
    "                        SELECT 1 FROM information_schema.tables\n",
    "                        WHERE table_schema='{schema}' AND table_name='{table}'\n",
    "                    ) THEN\n",
    "                        EXECUTE 'CREATE TABLE \"{schema}\".\"{table}\" ({create_cols_sql})';\n",
    "                    END IF;\n",
    "                END $$;\n",
    "            \"\"\"\n",
    "            )\n",
    "\n",
    "            # COPY from Polars CSV into target\n",
    "            csv_buf = io.StringIO()\n",
    "            df.write_csv(csv_buf, include_header=False)\n",
    "            csv_buf.seek(0)\n",
    "            cur.copy_expert(\n",
    "                f'COPY \"{schema}\".\"{table}\" ({collist_sql}) FROM STDIN WITH (FORMAT CSV)',\n",
    "                csv_buf,\n",
    "            )\n",
    "\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Process data\n",
    "# -------------------------\n",
    "def process(env):\n",
    "    \"\"\"Extract data from Dash API and load to S3 and Postgres.\"\"\"\n",
    "    # Setup logging\n",
    "    log = setup_logging()\n",
    "\n",
    "    # Get bearer token\n",
    "    bearer_token = get_bearer_token()\n",
    "\n",
    "    # Configure environment-specific settings\n",
    "    s3_bucket = f\"{S3_BUCKET}{env}\"\n",
    "    database_name = f\"{POSTGRES_DB}{env}\"\n",
    "    pg_conn = f\"dbname={database_name} user={POSTGRES_USER} password={POSTGRES_PASSWORD} host=localhost port=5432\"\n",
    "    keep_raw_jsonl = False\n",
    "    parquet_compress = \"snappy\"\n",
    "\n",
    "    log.info(f\"Starting extraction for environment: {env}\")\n",
    "    log.info(f\"Using S3 bucket: {s3_bucket}\")\n",
    "\n",
    "    # Fetch data from API\n",
    "    DASH_ENTITIES = [\n",
    "        \"event-types\",\n",
    "        \"events\",\n",
    "        \"customers\",\n",
    "        \"bookings\",\n",
    "        \"resources\",\n",
    "        \"stat-events\",\n",
    "        \"customer-events\",\n",
    "        \"addresses\",\n",
    "        \"invoices\",\n",
    "        \"event-comments\",\n",
    "        \"event-employees\",\n",
    "        \"customer-relationships\",\n",
    "    ]\n",
    "\n",
    "    DASH_ENTITIES = [\n",
    "        \"events\",\n",
    "    ]\n",
    "\n",
    "    for entity in DASH_ENTITIES:\n",
    "        try:\n",
    "            log.info(f\"Processing entity: {entity}\")\n",
    "            s3_target = f\"\"\"all_{entity.replace(\"-\",\"_\")}\"\"\"\n",
    "            db_schema = \"sch_raw\"\n",
    "            db_target = entity.replace(\"-\",\"_\")\n",
    "            records = get_index_data(entity, bearer_token)\n",
    "            log.info(f\"Fetched {len(records)} records from /v1/{entity}\")\n",
    "\n",
    "            # 2) Optional: write raw JSONL.gz to S3\n",
    "            run_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "            if keep_raw_jsonl and records:\n",
    "                raw_key = f\"raw/{s3_target}/ingest_date={run_date}/{s3_target}.jsonl.gz\"\n",
    "                log.info(f\"Writing raw JSONL to s3://{s3_bucket}/{raw_key}\")\n",
    "                write_jsonl_to_s3(records, s3_bucket, raw_key, s3_client)\n",
    "\n",
    "            # 3) Normalize to Polars\n",
    "            df = records_to_polars(records)\n",
    "            log.info(f\"Normalized to Polars: {df.height} rows, {len(df.columns)} columns\")\n",
    "\n",
    "            # 4) Write Parquet to S3 (replaced CSV)\n",
    "            parquet_key = f\"catalog/{s3_target}/ingest_date={run_date}/{s3_target}.parquet\"\n",
    "\n",
    "            log.info(f\"Writing Parquet to s3://{s3_bucket}/{parquet_key}\")\n",
    "            write_parquet_to_s3_polars(\n",
    "                df, s3_bucket, parquet_key, compression=parquet_compress, s3_client=s3_client\n",
    "            )\n",
    "\n",
    "            # 5) Replace table in Postgres\n",
    "            load_polars_df_to_postgres(\n",
    "                df, schema=db_schema, table=db_target, conn_str=pg_conn\n",
    "            )\n",
    "            log.info(f\"Replaced Postgres table {db_target}\")\n",
    "\n",
    "            break\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error processing entity: {entity}\")\n",
    "            raise e\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--env\",\n",
    "    default=\"dev\",\n",
    "    help=\"Environment (dev, prd)\",\n",
    "    type=click.Choice([\"dev\", \"prd\"]),\n",
    ")\n",
    "def main(env):\n",
    "    process(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd881bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 00:49:05 [INFO] dash_extract - Starting extraction for environment: dev\n",
      "2025-08-17 00:49:05 [INFO] dash_extract - Using S3 bucket: allstars-dl-us-west-2-dev\n",
      "2025-08-17 00:49:05 [INFO] dash_extract - Processing entity: events\n",
      "2025-08-17 00:49:15 [INFO] dash_extract - Fetched 2836 records from /v1/events\n",
      "2025-08-17 00:49:15 [INFO] botocore.credentials - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-17 00:49:15 [INFO] dash_extract - Normalized to Polars: 2836 rows, 42 columns\n",
      "2025-08-17 00:49:15 [INFO] dash_extract - Writing Parquet to s3://allstars-dl-us-west-2-dev/catalog/all_events/ingest_date=2025-08-17/all_events.parquet\n",
      "2025-08-17 00:49:16 [ERROR] dash_extract - Error processing entity: events\n"
     ]
    },
    {
     "ename": "ComputeError",
     "evalue": "CSV format does not support nested data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mComputeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdev\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 450\u001b[39m, in \u001b[36mprocess\u001b[39m\u001b[34m(env)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    449\u001b[39m     log.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError processing entity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 442\u001b[39m, in \u001b[36mprocess\u001b[39m\u001b[34m(env)\u001b[39m\n\u001b[32m    437\u001b[39m write_parquet_to_s3_polars(\n\u001b[32m    438\u001b[39m     df, s3_bucket, parquet_key, compression=parquet_compress, s3_client=s3_client\n\u001b[32m    439\u001b[39m )\n\u001b[32m    441\u001b[39m \u001b[38;5;66;03m# 5) Replace table in Postgres\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m \u001b[43mload_polars_df_to_postgres\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpg_conn\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m log.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReplaced Postgres table \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 355\u001b[39m, in \u001b[36mload_polars_df_to_postgres\u001b[39m\u001b[34m(df, table, conn_str, schema)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# COPY from Polars CSV into target\u001b[39;00m\n\u001b[32m    354\u001b[39m csv_buf = io.StringIO()\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_header\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m csv_buf.seek(\u001b[32m0\u001b[39m)\n\u001b[32m    357\u001b[39m cur.copy_expert(\n\u001b[32m    358\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCOPY \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollist_sql\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) FROM STDIN WITH (FORMAT CSV)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    359\u001b[39m     csv_buf,\n\u001b[32m    360\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/allstarscricket/allstars_dbt/.vdbt/lib/python3.12/site-packages/polars/dataframe/frame.py:3092\u001b[39m, in \u001b[36mDataFrame.write_csv\u001b[39m\u001b[34m(self, file, include_bom, include_header, separator, line_terminator, quote_char, batch_size, datetime_format, date_format, time_format, float_scientific, float_precision, decimal_comma, null_value, quote_style, storage_options, credential_provider, retries)\u001b[39m\n\u001b[32m   3088\u001b[39m engine: EngineType = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3090\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazyframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[32m-> \u001b[39m\u001b[32m3092\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msink_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3093\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3094\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_bom\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_bom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat_scientific\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfloat_scientific\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfloat_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnull_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnull_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquote_style\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquote_style\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredential_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredential_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3111\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryOptFlags\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3113\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m should_return_buffer:\n\u001b[32m   3116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(target.getvalue(), encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/allstarscricket/allstars_dbt/.vdbt/lib/python3.12/site-packages/polars/lazyframe/frame.py:3314\u001b[39m, in \u001b[36mLazyFrame.sink_csv\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3312\u001b[39m     ldf_py = ldf_py.with_optimizations(optimizations._pyoptflags)\n\u001b[32m   3313\u001b[39m     ldf = LazyFrame._from_pyldf(ldf_py)\n\u001b[32m-> \u001b[39m\u001b[32m3314\u001b[39m     \u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LazyFrame._from_pyldf(ldf_py)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/allstarscricket/allstars_dbt/.vdbt/lib/python3.12/site-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/allstarscricket/allstars_dbt/.vdbt/lib/python3.12/site-packages/polars/lazyframe/opt_flags.py:330\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    329\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/allstarscricket/allstars_dbt/.vdbt/lib/python3.12/site-packages/polars/lazyframe/frame.py:2335\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2333\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2334\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2335\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mComputeError\u001b[39m: CSV format does not support nested data"
     ]
    }
   ],
   "source": [
    "\n",
    "process(\"dev\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vdbt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
