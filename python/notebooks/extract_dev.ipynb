{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "import requests\n",
    "import click\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import psycopg2\n",
    "import polars as pl\n",
    "from polars.exceptions import ComputeError\n",
    "\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).parent.parent.resolve()\n",
    "    print(BASE_DIR)\n",
    "except NameError:\n",
    "    # Fallback for interactive sessions (no __file__)\n",
    "    BASE_DIR = Path(os.getcwd()).parent.parent.resolve()\n",
    "LOG_DIR = BASE_DIR / \"logs\"\n",
    "\n",
    "\n",
    "def setup_logging(\n",
    "    log_dir: str = LOG_DIR,\n",
    "    logger_name: str = \"dash_extract\",\n",
    "    when: str = \"midnight\",\n",
    "    interval: int = 1,\n",
    "    backup_count: int = 7,\n",
    "    level: int = logging.INFO,\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup logging with both console and time-based rotating file handlers.\n",
    "\n",
    "    Args:\n",
    "        log_dir (str): directory where logs are stored\n",
    "        log_file (str): base log file name\n",
    "        when (str): rotation interval type (e.g., 'S','M','H','D','midnight','W0'-'W6')\n",
    "        interval (int): number of intervals between rotations\n",
    "        backup_count (int): how many old logs to keep\n",
    "        level (int): logging level\n",
    "    \"\"\"\n",
    "    Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "    log_file = f\"{logger_name}.log\"\n",
    "    log_path = os.path.join(log_dir, log_file)\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    # Rotating file handler\n",
    "    file_handler = TimedRotatingFileHandler(\n",
    "        log_path,\n",
    "        when=when,\n",
    "        interval=interval,\n",
    "        backupCount=backup_count,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Root logger config\n",
    "    logging.basicConfig(level=level, handlers=[file_handler, console_handler])\n",
    "\n",
    "    return logging.getLogger(logger_name)\n",
    "\n",
    "\n",
    "def get_credentials():\n",
    "    with open(BASE_DIR / \"env.json\", \"r\") as file:\n",
    "        creds = json.load(file)\n",
    "    return creds\n",
    "\n",
    "\n",
    "credentials = get_credentials()\n",
    "\n",
    "DAYSMART_CLIENT_ID = credentials[\"DAYSMART_CLIENT_ID\"]\n",
    "DAYSMART_CLIENT_SECRET = credentials[\"DAYSMART_CLIENT_SECRET\"]\n",
    "DAYSMART_API_GRANT_TYPE = \"client_credentials\"\n",
    "POSTGRES_DB = credentials[\"POSTGRES_DB\"]\n",
    "POSTGRES_USER = credentials[\"POSTGRES_USER\"]\n",
    "POSTGRES_PASSWORD = credentials[\"POSTGRES_PASSWORD\"]\n",
    "S3_BUCKET = credentials[\"S3_BUCKET\"]\n",
    "BASE_URL = \"https://api.dashplatform.com\"\n",
    "LOG = setup_logging()\n",
    "\n",
    "\n",
    "def get_bearer_token():\n",
    "    url = BASE_URL + \"/v1/auth/token\"\n",
    "    headers = {\"Content-Type\": \"application/vnd.api+json\"}\n",
    "    payload = {\n",
    "        \"grant_type\": DAYSMART_API_GRANT_TYPE,\n",
    "        \"client_id\": DAYSMART_CLIENT_ID,\n",
    "        \"client_secret\": DAYSMART_CLIENT_SECRET,\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"access_token\")\n",
    "    else:\n",
    "        raise Exception(response.json())\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Fetcher\n",
    "# -------------------------\n",
    "def get_index_data(end_point, bearer_token):\n",
    "    url = BASE_URL + \"/v1/\" + end_point\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/vnd.api+json\",\n",
    "        \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "    }\n",
    "    params = {\"company\": \"allstars\", \"sort\": \"id\"}\n",
    "    all_data = []\n",
    "    next_url = url\n",
    "    while next_url:\n",
    "        response = requests.get(next_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # Append the current page's data (assuming it's under a 'data' key)\n",
    "            if \"data\" in data:\n",
    "                all_data.extend(data[\"data\"])\n",
    "\n",
    "            # Update the next_url for the next iteration\n",
    "            next_url = data.get(\"links\", {}).get(\"next\")\n",
    "        else:\n",
    "            raise Exception(response.json())\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# JSON → Polars\n",
    "# -------------------------\n",
    "def records_to_polars(records: list[dict]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Lifts JSON:API objects (id/type + attributes.*).\n",
    "    Builds a Polars DF with full-length schema inference.\n",
    "    If mixed types are detected across rows, falls back to string-cast.\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return pl.DataFrame([])\n",
    "\n",
    "    def lift_one(r: dict):\n",
    "        if isinstance(r, dict) and \"attributes\" in r:\n",
    "            base = {\n",
    "                k: v for k, v in r.items() if k not in (\"attributes\", \"relationships\")\n",
    "            }\n",
    "            base.update(r.get(\"attributes\", {}))\n",
    "            return base\n",
    "        return r\n",
    "\n",
    "    lifted = [lift_one(r) for r in records]\n",
    "\n",
    "    # First attempt: infer using full data scan\n",
    "    try:\n",
    "        df = pl.from_dicts(lifted, infer_schema_length=None)  # scan all rows\n",
    "    except ComputeError:\n",
    "        # Fallback: cast every value to string (JSON for nested), preserving None\n",
    "        def to_str(v):\n",
    "            if v is None:\n",
    "                return None\n",
    "            # stringify nested structures to preserve info for CSV/SQL\n",
    "            if isinstance(v, (dict, list)):\n",
    "                return json.dumps(v, separators=(\",\", \":\"), default=str)\n",
    "            return str(v)\n",
    "\n",
    "        lifted_str = [{k: to_str(v) for k, v in row.items()} for row in lifted]\n",
    "        df = pl.from_dicts(lifted_str, infer_schema_length=None)\n",
    "\n",
    "    # Unnest any 1-level struct columns if they exist (after first path)\n",
    "    struct_cols = [\n",
    "        c for c, dt in zip(df.columns, df.dtypes) if isinstance(dt, pl.Struct)\n",
    "    ]\n",
    "    for c in struct_cols:\n",
    "        df = df.unnest(c)\n",
    "\n",
    "    # Put id first if present\n",
    "    if \"id\" in df.columns:\n",
    "        df = df.select([\"id\", *[c for c in df.columns if c != \"id\"]])\n",
    "\n",
    "    now = datetime.now()  # timezone-aware UTC timestamp\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.lit(now).alias(\"inserted_dt\"),\n",
    "            pl.lit(now).alias(\"updated_dt\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# S3 Writers\n",
    "# -------------------------\n",
    "def write_jsonl_to_s3(records: list[dict], bucket: str, key: str, s3_client=None):\n",
    "    \"\"\"\n",
    "    Writes list-of-dicts as JSONL.gz to s3://bucket/key.\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    with gzip.GzipFile(fileobj=buf, mode=\"wb\") as gz:\n",
    "        for r in records:\n",
    "            line = (json.dumps(r, separators=(\",\", \":\"), default=str) + \"\\n\").encode(\n",
    "                \"utf-8\"\n",
    "            )\n",
    "            gz.write(line)\n",
    "    body = buf.getvalue()\n",
    "\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=body,\n",
    "            ContentType=\"application/json\",\n",
    "            ContentEncoding=\"gzip\",\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"S3 JSONL upload failed: {e}\")\n",
    "\n",
    "\n",
    "def write_csv_to_s3_polars(\n",
    "    df: pl.DataFrame,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    compress: bool = False,\n",
    "    include_header: bool = True,\n",
    "    s3_client=None,\n",
    "    **to_csv_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Write polars DataFrame to s3://bucket/key as CSV (optionally .gz).\n",
    "    to_csv_kwargs → passed to Polars write_csv (e.g., separator=',', quote='\\\"').\n",
    "    \"\"\"\n",
    "    if df.height == 0:\n",
    "        return\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "    # Polars write_csv expects a text buffer; wrap a BytesIO.\n",
    "    raw_buf = io.BytesIO()\n",
    "    text_buf = io.TextIOWrapper(raw_buf, encoding=\"utf-8\", newline=\"\")\n",
    "    df.write_csv(text_buf, include_header=include_header, **to_csv_kwargs)\n",
    "    text_buf.flush()\n",
    "    raw_bytes = raw_buf.getvalue()\n",
    "\n",
    "    extra = {\"ContentType\": \"text/csv; charset=utf-8\"}\n",
    "    if compress:\n",
    "        gz = io.BytesIO()\n",
    "        with gzip.GzipFile(fileobj=gz, mode=\"wb\") as z:\n",
    "            z.write(raw_bytes)\n",
    "        body = gz.getvalue()\n",
    "        extra[\"ContentEncoding\"] = \"gzip\"\n",
    "    else:\n",
    "        body = raw_bytes\n",
    "\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=bucket, Key=key, Body=body, **extra)\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"S3 CSV upload failed: {e}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Postgres Loader (TRUNCATE + INSERT)\n",
    "# -------------------------\n",
    "def load_polars_df_to_postgres(\n",
    "    df: pl.DataFrame,\n",
    "    table: str,\n",
    "    conn_str: str,\n",
    "    schema: str = \"public\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Replaces target table: TRUNCATE then bulk INSERT via COPY.\n",
    "    Creates table if absent (all columns TEXT for simplicity).\n",
    "    \"\"\"\n",
    "    if df.height == 0:\n",
    "        LOG.info(\"No rows; skipping Postgres replace.\")\n",
    "        return\n",
    "\n",
    "    cols = df.columns\n",
    "    collist_sql = \", \".join([f'\"{c}\"' for c in cols])\n",
    "\n",
    "    conn = psycopg2.connect(conn_str)\n",
    "    conn.autocommit = False\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Create table if missing (all TEXT columns)\n",
    "            create_cols_sql = \", \".join([f'\"{c}\" text' for c in cols])\n",
    "\n",
    "            # Create schema\n",
    "            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n",
    "            # DROP target\n",
    "            cur.execute(f'DROP TABLE IF EXISTS \"{schema}\".\"{table}\";')\n",
    "            cur.execute(\n",
    "                f\"\"\"\n",
    "                DO $$\n",
    "                BEGIN\n",
    "                    IF NOT EXISTS (\n",
    "                        SELECT 1 FROM information_schema.tables\n",
    "                        WHERE table_schema='{schema}' AND table_name='{table}'\n",
    "                    ) THEN\n",
    "                        EXECUTE 'CREATE TABLE \"{schema}\".\"{table}\" ({create_cols_sql})';\n",
    "                    END IF;\n",
    "                END $$;\n",
    "            \"\"\"\n",
    "            )\n",
    "\n",
    "            # COPY from Polars CSV into target\n",
    "            csv_buf = io.StringIO()\n",
    "            df.write_csv(csv_buf, include_header=False)\n",
    "            csv_buf.seek(0)\n",
    "            cur.copy_expert(\n",
    "                f'COPY \"{schema}\".\"{table}\" ({collist_sql}) FROM STDIN WITH (FORMAT CSV)',\n",
    "                csv_buf,\n",
    "            )\n",
    "\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--env\",\n",
    "    default=\"dev\",\n",
    "    help=\"Environment (dev, staging, prod)\",\n",
    "    type=click.Choice([\"dev\", \"staging\", \"prod\"]),\n",
    ")\n",
    "def main(env):\n",
    "    \"\"\"Extract data from Dash API and load to S3 and Postgres.\"\"\"\n",
    "    # Setup logging\n",
    "    log = setup_logging()\n",
    "\n",
    "    # Get bearer token\n",
    "    bearer_token = get_bearer_token()\n",
    "\n",
    "    # Configure environment-specific settings\n",
    "    s3_bucket = f\"{S3_BUCKET}{env}\"\n",
    "    pg_conn = f\"dbname={POSTGRES_DB} user={POSTGRES_USER} password={POSTGRES_PASSWORD} host=localhost port=5432\"\n",
    "    keep_raw_jsonl = False\n",
    "    csv_compress = True\n",
    "\n",
    "    log.info(f\"Starting extraction for environment: {env}\")\n",
    "    log.info(f\"Using S3 bucket: {s3_bucket}\")\n",
    "\n",
    "    # Fetch data from API\n",
    "    DASH_ENTITIES = [\n",
    "        \"event-types\",\n",
    "        \"events\",\n",
    "        \"customers\",\n",
    "        \"bookings\",\n",
    "        \"resources\",\n",
    "        \"stat-events\",\n",
    "        \"customer-events\",\n",
    "        \"addresses\",\n",
    "        \"invoices\",\n",
    "        \"event-comments\",\n",
    "        \"event-employees\",\n",
    "        \"customer-relationships\",\n",
    "    ]\n",
    "\n",
    "    for entity in DASH_ENTITIES:\n",
    "    \n",
    "        s3_target = f\"\"\"all_{entity.replace(\"-\",\"_\")}\"\"\"\n",
    "        db_schema = \"sch_raw\"\n",
    "        db_target = s3_target\n",
    "        records = get_index_data(entity, bearer_token)\n",
    "        log.info(f\"Fetched {len(records)} records from /v1/{entity}\")\n",
    "\n",
    "        # 2) Optional: write raw JSONL.gz to S3\n",
    "        run_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "        if keep_raw_jsonl and records:\n",
    "            raw_key = f\"raw/{s3_target}/ingest_date={run_date}/{s3_target}.jsonl.gz\"\n",
    "            log.info(f\"Writing raw JSONL to s3://{s3_bucket}/{raw_key}\")\n",
    "            write_jsonl_to_s3(records, s3_bucket, raw_key, s3_client)\n",
    "\n",
    "        # 3) Normalize to Polars\n",
    "        df = records_to_polars(records)\n",
    "        log.info(f\"Normalized to Polars: {df.height} rows, {len(df.columns)} columns\")\n",
    "\n",
    "        # 4) Write CSV to S3\n",
    "        csv_key = f\"catalog/{s3_target}/ingest_date={run_date}/{s3_target}.csv\"\n",
    "        if csv_compress:\n",
    "            csv_key += \".gz\"\n",
    "\n",
    "        log.info(f\"Writing CSV to s3://{s3_bucket}/{csv_key}\")\n",
    "        write_csv_to_s3_polars(\n",
    "            df, s3_bucket, csv_key, compress=csv_compress, s3_client=s3_client\n",
    "        )\n",
    "\n",
    "        # 5) Replace table in Postgres\n",
    "        load_polars_df_to_postgres(\n",
    "            df, schema=db_schema, table=db_target, conn_str=pg_conn\n",
    "        )\n",
    "        log.info(f\"Replaced Postgres table {db_target}\")\n",
    "\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "975da893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: ipykernel_launcher.py [OPTIONS]\n",
      "Try 'ipykernel_launcher.py --help' for help.\n",
      "\n",
      "Error: Got unexpected extra arguments (d e v)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csarat/git/allstarscricket/allstars_dbt/.vdbt/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main(\"dev\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vdbt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
