{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d75b5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "import requests\n",
    "import click\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import psycopg2\n",
    "import polars as pl\n",
    "from polars.exceptions import ComputeError\n",
    "\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).parent.parent.parent.resolve()\n",
    "except NameError:\n",
    "    # Fallback for interactive sessions (no __file__)\n",
    "    BASE_DIR = Path(os.getcwd()).parent.parent.resolve()\n",
    "LOG_DIR = BASE_DIR / \"logs\"\n",
    "\n",
    "\n",
    "def setup_logging(\n",
    "    log_dir: str = LOG_DIR,\n",
    "    logger_name: str = \"dash_extract\",\n",
    "    when: str = \"midnight\",\n",
    "    interval: int = 1,\n",
    "    backup_count: int = 7,\n",
    "    level: int = logging.INFO,\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup logging with both console and time-based rotating file handlers.\n",
    "\n",
    "    Args:\n",
    "        log_dir (str): directory where logs are stored\n",
    "        log_file (str): base log file name\n",
    "        when (str): rotation interval type (e.g., 'S','M','H','D','midnight','W0'-'W6')\n",
    "        interval (int): number of intervals between rotations\n",
    "        backup_count (int): how many old logs to keep\n",
    "        level (int): logging level\n",
    "    \"\"\"\n",
    "    Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "    log_file = f\"{logger_name}.log\"\n",
    "    log_path = os.path.join(log_dir, log_file)\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    # Rotating file handler\n",
    "    file_handler = TimedRotatingFileHandler(\n",
    "        log_path,\n",
    "        when=when,\n",
    "        interval=interval,\n",
    "        backupCount=backup_count,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Root logger config\n",
    "    logging.basicConfig(level=level, handlers=[file_handler, console_handler])\n",
    "\n",
    "    return logging.getLogger(logger_name)\n",
    "\n",
    "\n",
    "def get_credentials():\n",
    "    with open(BASE_DIR / \"env.json\", \"r\") as file:\n",
    "        creds = json.load(file)\n",
    "    return creds\n",
    "\n",
    "\n",
    "credentials = get_credentials()\n",
    "\n",
    "DAYSMART_CLIENT_ID = credentials[\"DAYSMART_CLIENT_ID\"]\n",
    "DAYSMART_CLIENT_SECRET = credentials[\"DAYSMART_CLIENT_SECRET\"]\n",
    "DAYSMART_API_GRANT_TYPE = \"client_credentials\"\n",
    "POSTGRES_DB = credentials[\"POSTGRES_DB\"]\n",
    "POSTGRES_USER = credentials[\"POSTGRES_USER\"]\n",
    "POSTGRES_PASSWORD = credentials[\"POSTGRES_PASSWORD\"]\n",
    "S3_BUCKET = credentials[\"S3_BUCKET\"]\n",
    "BASE_URL = \"https://api.dashplatform.com\"\n",
    "LOG = setup_logging()\n",
    "\n",
    "\n",
    "def get_bearer_token():\n",
    "    url = BASE_URL + \"/v1/auth/token\"\n",
    "    headers = {\"Content-Type\": \"application/vnd.api+json\"}\n",
    "    payload = {\n",
    "        \"grant_type\": DAYSMART_API_GRANT_TYPE,\n",
    "        \"client_id\": DAYSMART_CLIENT_ID,\n",
    "        \"client_secret\": DAYSMART_CLIENT_SECRET,\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"access_token\")\n",
    "    else:\n",
    "        raise Exception(response.json())\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Fetcher\n",
    "# -------------------------\n",
    "def get_index_data(end_point, bearer_token):\n",
    "    url = BASE_URL + \"/v1/\" + end_point\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/vnd.api+json\",\n",
    "        \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "    }\n",
    "    params = {\"company\": \"allstars\", \"sort\": \"id\"}\n",
    "    all_data = []\n",
    "    next_url = url\n",
    "    while next_url:\n",
    "        response = requests.get(next_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # Append the current page's data (assuming it's under a 'data' key)\n",
    "            if \"data\" in data:\n",
    "                all_data.extend(data[\"data\"])\n",
    "\n",
    "            # Update the next_url for the next iteration\n",
    "            next_url = data.get(\"links\", {}).get(\"next\")\n",
    "        else:\n",
    "            raise Exception(response.json())\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# JSON → Polars\n",
    "# -------------------------\n",
    "def records_to_polars(records: list[dict]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Lifts JSON:API objects (id/type + attributes.*).\n",
    "    Builds a Polars DF with full-length schema inference.\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return pl.DataFrame([])\n",
    "\n",
    "    def lift_one(r: dict):\n",
    "        if isinstance(r, dict) and \"attributes\" in r:\n",
    "            base = {\n",
    "                k: v for k, v in r.items() if k not in (\"attributes\", \"relationships\")\n",
    "            }\n",
    "            base.update(r.get(\"attributes\", {}))\n",
    "            return base\n",
    "        return r\n",
    "\n",
    "    lifted = [lift_one(r) for r in records]\n",
    "\n",
    "    # ALWAYS convert nested structures to JSON strings to preserve key-value pairs\n",
    "    def process_nested_values(row):\n",
    "        processed_row = {}\n",
    "        for k, v in row.items():\n",
    "            if v is None:\n",
    "                processed_row[k] = None\n",
    "            elif isinstance(v, (dict, list)):\n",
    "                # Convert nested structures to JSON strings to preserve structure\n",
    "                processed_row[k] = json.dumps(v, separators=(\",\", \":\"), default=str)\n",
    "            else:\n",
    "                processed_row[k] = v\n",
    "        return processed_row\n",
    "\n",
    "    # Process all rows to convert nested structures to JSON strings\n",
    "    processed_lifted = [process_nested_values(row) for row in lifted]\n",
    "\n",
    "    # Create DataFrame with processed data\n",
    "    df = pl.from_dicts(processed_lifted, infer_schema_length=None)\n",
    "\n",
    "    # Put id first if present\n",
    "    if \"id\" in df.columns:\n",
    "        df = df.select([\"id\", *[c for c in df.columns if c != \"id\"]])\n",
    "\n",
    "    now = datetime.now()  # timezone-aware UTC timestamp\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.lit(now).alias(\"inserted_dt\"),\n",
    "            pl.lit(now).alias(\"updated_dt\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# S3 Writers\n",
    "# -------------------------\n",
    "def write_jsonl_to_s3(records: list[dict], bucket: str, key: str, s3_client=None):\n",
    "    \"\"\"\n",
    "    Writes list-of-dicts as JSONL.gz to s3://bucket/key.\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    with gzip.GzipFile(fileobj=buf, mode=\"wb\") as gz:\n",
    "        for r in records:\n",
    "            line = (json.dumps(r, separators=(\",\", \":\"), default=str) + \"\\n\").encode(\n",
    "                \"utf-8\"\n",
    "            )\n",
    "            gz.write(line)\n",
    "    body = buf.getvalue()\n",
    "\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=body,\n",
    "            ContentType=\"application/json\",\n",
    "            ContentEncoding=\"gzip\",\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"S3 JSONL upload failed: {e}\")\n",
    "\n",
    "\n",
    "def write_csv_to_s3_polars(\n",
    "    df: pl.DataFrame,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    compress: bool = False,\n",
    "    include_header: bool = True,\n",
    "    s3_client=None,\n",
    "    **to_csv_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Write polars DataFrame to s3://bucket/key as CSV (optionally .gz).\n",
    "    to_csv_kwargs → passed to Polars write_csv (e.g., separator=',', quote='\\\"').\n",
    "    \"\"\"\n",
    "    if df.height == 0:\n",
    "        return\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "    # Polars write_csv expects a text buffer; wrap a BytesIO.\n",
    "    raw_buf = io.BytesIO()\n",
    "    text_buf = io.TextIOWrapper(raw_buf, encoding=\"utf-8\", newline=\"\")\n",
    "    df.write_csv(text_buf, include_header=include_header, **to_csv_kwargs)\n",
    "    text_buf.flush()\n",
    "    raw_bytes = raw_buf.getvalue()\n",
    "\n",
    "    extra = {\"ContentType\": \"text/csv; charset=utf-8\"}\n",
    "    if compress:\n",
    "        gz = io.BytesIO()\n",
    "        with gzip.GzipFile(fileobj=gz, mode=\"wb\") as z:\n",
    "            z.write(raw_bytes)\n",
    "        body = gz.getvalue()\n",
    "        extra[\"ContentEncoding\"] = \"gzip\"\n",
    "    else:\n",
    "        body = raw_bytes\n",
    "\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=bucket, Key=key, Body=body, **extra)\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"S3 CSV upload failed: {e}\")\n",
    "\n",
    "\n",
    "def write_parquet_to_s3_polars(\n",
    "    df: pl.DataFrame,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    compression: str = \"snappy\",\n",
    "    s3_client=None,\n",
    "    **to_parquet_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Write polars DataFrame to s3://bucket/key as Parquet.\n",
    "    to_parquet_kwargs → passed to Polars write_parquet (e.g., compression='snappy').\n",
    "    \"\"\"\n",
    "    if df.height == 0:\n",
    "        return\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "    # Write DataFrame to bytes buffer\n",
    "    buf = io.BytesIO()\n",
    "    df.write_parquet(buf, compression=compression, **to_parquet_kwargs)\n",
    "    body = buf.getvalue()\n",
    "\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=body,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"S3 Parquet upload failed: {e}\")\n",
    "\n",
    "\n",
    "def write_parquet_to_s3_polars_native(\n",
    "    df: pl.DataFrame,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    compression: str = \"snappy\",\n",
    "    **to_parquet_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Write polars DataFrame directly to s3://bucket/key as Parquet using native S3 support.\n",
    "    This is more memory efficient as it doesn't buffer the entire file in memory.\n",
    "    \"\"\"\n",
    "    if df.height == 0:\n",
    "        return\n",
    "    \n",
    "    # Construct S3 path\n",
    "    s3_path = f\"s3://{bucket}/{key}\"\n",
    "    \n",
    "    try:\n",
    "        # Write directly to S3\n",
    "        df.write_parquet(\n",
    "            s3_path, \n",
    "            compression=compression, \n",
    "            **to_parquet_kwargs\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"S3 Parquet upload failed: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# Postgres Loader (TRUNCATE + INSERT)\n",
    "# -------------------------\n",
    "def load_polars_df_to_postgres(\n",
    "    df: pl.DataFrame,\n",
    "    table: str,\n",
    "    conn_str: str,\n",
    "    schema: str = \"public\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Replaces target table: TRUNCATE then bulk INSERT via Polars write_to_database.\n",
    "    Creates table if absent (all columns TEXT for simplicity).\n",
    "    \"\"\"\n",
    "    if df.height == 0:\n",
    "        LOG.info(\"No rows; skipping Postgres replace.\")\n",
    "        return\n",
    "\n",
    "    # Convert psycopg2 connection string to URL format\n",
    "    conn_parts = {}\n",
    "    for part in conn_str.split():\n",
    "        if '=' in part:\n",
    "            key, value = part.split('=', 1)\n",
    "            conn_parts[key] = value\n",
    "    \n",
    "    # Create PostgreSQL URL\n",
    "    pg_url = f\"postgresql://{conn_parts['user']}:{conn_parts['password']}@{conn_parts['host']}:{conn_parts['port']}/{conn_parts['dbname']}\"\n",
    "    \n",
    "    log.info(f\"pg_url={pg_url}\")\n",
    "\n",
    "    conn = psycopg2.connect(conn_str)\n",
    "    conn.autocommit = True\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Create schema\n",
    "            cur.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n",
    "            # DROP target\n",
    "            cur.execute(f'DROP TABLE IF EXISTS \"{schema}\".\"{table}\";')\n",
    "            \n",
    "            # Create table with proper schema inference\n",
    "            # Let Polars handle the table creation with proper data types\n",
    "            df.write_database(\n",
    "                table_name=f\"{schema}.{table}\",\n",
    "                connection=pg_url,\n",
    "                if_table_exists=\"replace\"\n",
    "            )\n",
    "\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bcb29fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 23:27:04 [INFO] dash_extract - Starting extraction for environment: dev\n",
      "2025-08-17 23:27:04 [INFO] dash_extract - Using S3 bucket: allstars-dl-us-west-2-dev\n",
      "2025-08-17 23:27:04 [INFO] dash_extract - Processing entity: events\n",
      "2025-08-17 23:27:13 [INFO] dash_extract - Fetched 2901 records from /v1/events\n"
     ]
    }
   ],
   "source": [
    "env = \"dev\"\n",
    "\"\"\"Extract data from Dash API and load to S3 and Postgres.\"\"\"\n",
    "# Setup logging\n",
    "log = setup_logging()\n",
    "\n",
    "# Get bearer token\n",
    "bearer_token = get_bearer_token()\n",
    "\n",
    "# Configure environment-specific settings\n",
    "s3_bucket = f\"{S3_BUCKET}{env}\"\n",
    "database_name = f\"{POSTGRES_DB}{env}\"\n",
    "pg_conn = f\"dbname={database_name} user={POSTGRES_USER} password={POSTGRES_PASSWORD} host=localhost port=5432\"\n",
    "keep_raw_jsonl = False\n",
    "csv_compress = True\n",
    "\n",
    "log.info(f\"Starting extraction for environment: {env}\")\n",
    "log.info(f\"Using S3 bucket: {s3_bucket}\")\n",
    "\n",
    "\n",
    "entity = \"events\"\n",
    "log.info(f\"Processing entity: {entity}\")\n",
    "s3_target = f\"\"\"all_{entity.replace(\"-\",\"_\")}\"\"\"\n",
    "db_schema = \"sch_raw\"\n",
    "db_target = entity.replace(\"-\",\"_\")\n",
    "records = get_index_data(entity, bearer_token)\n",
    "log.info(f\"Fetched {len(records)} records from /v1/{entity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe6ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [{'type': 'events',\n",
    "  'id': '2',\n",
    "  'attributes': {'repeat_id': 2,\n",
    "   'resource_id': 3,\n",
    "   'resource_area_id': 0,\n",
    "   'desc': 'Closed for renevation',\n",
    "   'event_type_id': 'b',\n",
    "   'sub_type': 'regular',\n",
    "   'start': '2025-02-28T07:00:00',\n",
    "   'start_gmt': '2025-02-28 15:00:00',\n",
    "   'end': '2025-02-28T12:00:00',\n",
    "   'end_gmt': '2025-02-28 20:00:00',\n",
    "   'customer_id': 0,\n",
    "   'hteam_id': None,\n",
    "   'vteam_id': None,\n",
    "   'league_id': None,\n",
    "   'home_score': None,\n",
    "   'visiting_score': None,\n",
    "   'publish': False,\n",
    "   'outcome': '',\n",
    "   'register_capacity': 0,\n",
    "   'create_u': 'root',\n",
    "   'created_user_type': 'SIT_Employee',\n",
    "   'create_d': '2025-02-28T11:43:30',\n",
    "   'mod_u': 'root',\n",
    "   'last_modified_user_type': 'SIT_Employee',\n",
    "   'mod_d': '2025-02-28T11:45:13',\n",
    "   'is_overtime': False,\n",
    "   'booking_id': None,\n",
    "   'description': None,\n",
    "   'notice': None,\n",
    "   'last_resource_id': None,\n",
    "   'parent_event_id': None,\n",
    "   'has_gender_locker_rooms': 0,\n",
    "   'locker_room_type': None,\n",
    "   'includes_setup_time': False,\n",
    "   'includes_takedown_time': False,\n",
    "   'start_date': '2025-02-28T00:00:00',\n",
    "   'event_start_time': '07:00:00',\n",
    "   'best_description': None},\n",
    "  'relationships': {'customer': [],\n",
    "   'registrants': [],\n",
    "   'registrations': [],\n",
    "   'eventType': [],\n",
    "   'homeTeam': [],\n",
    "   'visitingTeam': [],\n",
    "   'summary': [],\n",
    "   'league': [],\n",
    "   'booking': [],\n",
    "   'parentEvent': [],\n",
    "   'lockers': [],\n",
    "   'lastResource': [],\n",
    "   'resource': [],\n",
    "   'resourceArea': [],\n",
    "   'tasks': [],\n",
    "   'teamGroups': [],\n",
    "   'eventSeries': [],\n",
    "   'statEvents': [],\n",
    "   'fees': [],\n",
    "   'invoices': [],\n",
    "   'seriesInvoices': [],\n",
    "   'invoiceItems': [],\n",
    "   'employees': [],\n",
    "   'eventEmployees': [],\n",
    "   'additionalResources': [],\n",
    "   'setupEvents': [],\n",
    "   'takedownEvents': [],\n",
    "   'comments': [],\n",
    "   'rsvpStates': []}},\n",
    " {'type': 'events',\n",
    "  'id': '307',\n",
    "  'attributes': {'repeat_id': 306,\n",
    "   'resource_id': 4,\n",
    "   'resource_area_id': 0,\n",
    "   'desc': '',\n",
    "   'event_type_id': 'k',\n",
    "   'sub_type': 'regular',\n",
    "   'start': '2025-05-27T13:00:00',\n",
    "   'start_gmt': '2025-05-27 18:00:00',\n",
    "   'end': '2025-05-27T16:00:00',\n",
    "   'end_gmt': '2025-05-27 21:00:00',\n",
    "   'customer_id': 0,\n",
    "   'hteam_id': 7,\n",
    "   'vteam_id': None,\n",
    "   'league_id': 3,\n",
    "   'home_score': None,\n",
    "   'visiting_score': None,\n",
    "   'publish': True,\n",
    "   'outcome': '',\n",
    "   'register_capacity': 0,\n",
    "   'create_u': 'csarat',\n",
    "   'created_user_type': 'SIT_Employee',\n",
    "   'create_d': '2025-04-22T16:55:15',\n",
    "   'mod_u': 'neelak',\n",
    "   'last_modified_user_type': 'SIT_Employee',\n",
    "   'mod_d': '2025-05-26T22:37:33',\n",
    "   'is_overtime': False,\n",
    "   'booking_id': None,\n",
    "   'description': None,\n",
    "   'notice': None,\n",
    "   'last_resource_id': None,\n",
    "   'parent_event_id': None,\n",
    "   'has_gender_locker_rooms': 0,\n",
    "   'locker_room_type': None,\n",
    "   'includes_setup_time': False,\n",
    "   'includes_takedown_time': False,\n",
    "   'start_date': '2025-05-27T00:00:00',\n",
    "   'event_start_time': '13:00:00',\n",
    "   'best_description': {'league_id': 3,\n",
    "    'has_morning_events': True,\n",
    "    'has_afternoon_events': True,\n",
    "    'has_evening_events': False,\n",
    "    'team_count': 4}},\n",
    "  'relationships': {'customer': [],\n",
    "   'registrants': [],\n",
    "   'registrations': [],\n",
    "   'eventType': [],\n",
    "   'homeTeam': [],\n",
    "   'visitingTeam': [],\n",
    "   'summary': [],\n",
    "   'league': [],\n",
    "   'booking': [],\n",
    "   'parentEvent': [],\n",
    "   'lockers': [],\n",
    "   'lastResource': [],\n",
    "   'resource': [],\n",
    "   'resourceArea': [],\n",
    "   'tasks': [],\n",
    "   'teamGroups': [],\n",
    "   'eventSeries': [],\n",
    "   'statEvents': [],\n",
    "   'fees': [],\n",
    "   'invoices': [],\n",
    "   'seriesInvoices': [],\n",
    "   'invoiceItems': [],\n",
    "   'employees': [],\n",
    "   'eventEmployees': [],\n",
    "   'additionalResources': [],\n",
    "   'setupEvents': [],\n",
    "   'takedownEvents': [],\n",
    "   'comments': [],\n",
    "   'rsvpStates': []}},\n",
    " {'type': 'events',\n",
    "  'id': '308',\n",
    "  'attributes': {'repeat_id': 306,\n",
    "   'resource_id': 5,\n",
    "   'resource_area_id': 0,\n",
    "   'desc': '',\n",
    "   'event_type_id': 'k',\n",
    "   'sub_type': 'regular',\n",
    "   'start': '2025-05-28T13:00:00',\n",
    "   'start_gmt': '2025-05-28 18:00:00',\n",
    "   'end': '2025-05-28T16:00:00',\n",
    "   'end_gmt': '2025-05-28 21:00:00',\n",
    "   'customer_id': 0,\n",
    "   'hteam_id': 7,\n",
    "   'vteam_id': None,\n",
    "   'league_id': 3,\n",
    "   'home_score': None,\n",
    "   'visiting_score': None,\n",
    "   'publish': True,\n",
    "   'outcome': '',\n",
    "   'register_capacity': 0,\n",
    "   'create_u': 'csarat',\n",
    "   'created_user_type': 'SIT_Employee',\n",
    "   'create_d': '2025-04-22T16:55:15',\n",
    "   'mod_u': 'csarat',\n",
    "   'last_modified_user_type': 'SIT_Employee',\n",
    "   'mod_d': '2025-04-26T15:31:12',\n",
    "   'is_overtime': False,\n",
    "   'booking_id': None,\n",
    "   'description': None,\n",
    "   'notice': None,\n",
    "   'last_resource_id': None,\n",
    "   'parent_event_id': None,\n",
    "   'has_gender_locker_rooms': 0,\n",
    "   'locker_room_type': None,\n",
    "   'includes_setup_time': False,\n",
    "   'includes_takedown_time': False,\n",
    "   'start_date': '2025-05-28T00:00:00',\n",
    "   'event_start_time': '13:00:00',\n",
    "   'best_description': {'league_id': 3,\n",
    "    'has_morning_events': True,\n",
    "    'has_afternoon_events': True,\n",
    "    'has_evening_events': False,\n",
    "    'team_count': 4}},\n",
    "  'relationships': {'customer': [],\n",
    "   'registrants': [],\n",
    "   'registrations': [],\n",
    "   'eventType': [],\n",
    "   'homeTeam': [],\n",
    "   'visitingTeam': [],\n",
    "   'summary': [],\n",
    "   'league': [],\n",
    "   'booking': [],\n",
    "   'parentEvent': [],\n",
    "   'lockers': [],\n",
    "   'lastResource': [],\n",
    "   'resource': [],\n",
    "   'resourceArea': [],\n",
    "   'tasks': [],\n",
    "   'teamGroups': [],\n",
    "   'eventSeries': [],\n",
    "   'statEvents': [],\n",
    "   'fees': [],\n",
    "   'invoices': [],\n",
    "   'seriesInvoices': [],\n",
    "   'invoiceItems': [],\n",
    "   'employees': [],\n",
    "   'eventEmployees': [],\n",
    "   'additionalResources': [],\n",
    "   'setupEvents': [],\n",
    "   'takedownEvents': [],\n",
    "   'comments': [],\n",
    "   'rsvpStates': []}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98c25c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 23:45:28 [INFO] botocore.credentials - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-17 23:45:28 [INFO] dash_extract - Normalized to Polars: 3 rows, 42 columns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Optional: write raw JSONL.gz to S3\n",
    "run_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "s3_client = boto3.Session(profile_name=\"etl_user\").client(\"s3\")\n",
    "\n",
    "# if keep_raw_jsonl and records:\n",
    "#     raw_key = f\"raw/{s3_target}/ingest_date={run_date}/{s3_target}.jsonl.gz\"\n",
    "#     log.info(f\"Writing raw JSONL to s3://{s3_bucket}/{raw_key}\")\n",
    "#     write_jsonl_to_s3(records, s3_bucket, raw_key, s3_client)\n",
    "\n",
    "# 3) Normalize to Polars\n",
    "df = records_to_polars(records)\n",
    "log.info(f\"Normalized to Polars: {df.height} rows, {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cca91e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 42)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>type</th><th>repeat_id</th><th>resource_id</th><th>resource_area_id</th><th>desc</th><th>event_type_id</th><th>sub_type</th><th>start</th><th>start_gmt</th><th>end</th><th>end_gmt</th><th>customer_id</th><th>hteam_id</th><th>vteam_id</th><th>league_id</th><th>home_score</th><th>visiting_score</th><th>publish</th><th>outcome</th><th>register_capacity</th><th>create_u</th><th>created_user_type</th><th>create_d</th><th>mod_u</th><th>last_modified_user_type</th><th>mod_d</th><th>is_overtime</th><th>booking_id</th><th>description</th><th>notice</th><th>last_resource_id</th><th>parent_event_id</th><th>has_gender_locker_rooms</th><th>locker_room_type</th><th>includes_setup_time</th><th>includes_takedown_time</th><th>start_date</th><th>event_start_time</th><th>best_description</th><th>inserted_dt</th><th>updated_dt</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>null</td><td>i64</td><td>null</td><td>null</td><td>bool</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>bool</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>i64</td><td>null</td><td>bool</td><td>bool</td><td>str</td><td>str</td><td>str</td><td>datetime[μs]</td><td>datetime[μs]</td></tr></thead><tbody><tr><td>&quot;2&quot;</td><td>&quot;events&quot;</td><td>2</td><td>3</td><td>0</td><td>&quot;Closed for renevation&quot;</td><td>&quot;b&quot;</td><td>&quot;regular&quot;</td><td>&quot;2025-02-28T07:00:00&quot;</td><td>&quot;2025-02-28 15:00:00&quot;</td><td>&quot;2025-02-28T12:00:00&quot;</td><td>&quot;2025-02-28 20:00:00&quot;</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>false</td><td>&quot;&quot;</td><td>0</td><td>&quot;root&quot;</td><td>&quot;SIT_Employee&quot;</td><td>&quot;2025-02-28T11:43:30&quot;</td><td>&quot;root&quot;</td><td>&quot;SIT_Employee&quot;</td><td>&quot;2025-02-28T11:45:13&quot;</td><td>false</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>false</td><td>false</td><td>&quot;2025-02-28T00:00:00&quot;</td><td>&quot;07:00:00&quot;</td><td>null</td><td>2025-08-17 23:45:28.568608</td><td>2025-08-17 23:45:28.568608</td></tr><tr><td>&quot;307&quot;</td><td>&quot;events&quot;</td><td>306</td><td>4</td><td>0</td><td>&quot;&quot;</td><td>&quot;k&quot;</td><td>&quot;regular&quot;</td><td>&quot;2025-05-27T13:00:00&quot;</td><td>&quot;2025-05-27 18:00:00&quot;</td><td>&quot;2025-05-27T16:00:00&quot;</td><td>&quot;2025-05-27 21:00:00&quot;</td><td>0</td><td>7</td><td>null</td><td>3</td><td>null</td><td>null</td><td>true</td><td>&quot;&quot;</td><td>0</td><td>&quot;csarat&quot;</td><td>&quot;SIT_Employee&quot;</td><td>&quot;2025-04-22T16:55:15&quot;</td><td>&quot;neelak&quot;</td><td>&quot;SIT_Employee&quot;</td><td>&quot;2025-05-26T22:37:33&quot;</td><td>false</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>false</td><td>false</td><td>&quot;2025-05-27T00:00:00&quot;</td><td>&quot;13:00:00&quot;</td><td>&quot;{&quot;league_id&quot;:3,&quot;has_morning_ev…</td><td>2025-08-17 23:45:28.568608</td><td>2025-08-17 23:45:28.568608</td></tr><tr><td>&quot;308&quot;</td><td>&quot;events&quot;</td><td>306</td><td>5</td><td>0</td><td>&quot;&quot;</td><td>&quot;k&quot;</td><td>&quot;regular&quot;</td><td>&quot;2025-05-28T13:00:00&quot;</td><td>&quot;2025-05-28 18:00:00&quot;</td><td>&quot;2025-05-28T16:00:00&quot;</td><td>&quot;2025-05-28 21:00:00&quot;</td><td>0</td><td>7</td><td>null</td><td>3</td><td>null</td><td>null</td><td>true</td><td>&quot;&quot;</td><td>0</td><td>&quot;csarat&quot;</td><td>&quot;SIT_Employee&quot;</td><td>&quot;2025-04-22T16:55:15&quot;</td><td>&quot;csarat&quot;</td><td>&quot;SIT_Employee&quot;</td><td>&quot;2025-04-26T15:31:12&quot;</td><td>false</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>false</td><td>false</td><td>&quot;2025-05-28T00:00:00&quot;</td><td>&quot;13:00:00&quot;</td><td>&quot;{&quot;league_id&quot;:3,&quot;has_morning_ev…</td><td>2025-08-17 23:45:28.568608</td><td>2025-08-17 23:45:28.568608</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 42)\n",
       "┌─────┬────────┬───────────┬─────────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n",
       "│ id  ┆ type   ┆ repeat_id ┆ resource_id ┆ … ┆ event_start ┆ best_descri ┆ inserted_d ┆ updated_dt │\n",
       "│ --- ┆ ---    ┆ ---       ┆ ---         ┆   ┆ _time       ┆ ption       ┆ t          ┆ ---        │\n",
       "│ str ┆ str    ┆ i64       ┆ i64         ┆   ┆ ---         ┆ ---         ┆ ---        ┆ datetime[μ │\n",
       "│     ┆        ┆           ┆             ┆   ┆ str         ┆ str         ┆ datetime[μ ┆ s]         │\n",
       "│     ┆        ┆           ┆             ┆   ┆             ┆             ┆ s]         ┆            │\n",
       "╞═════╪════════╪═══════════╪═════════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n",
       "│ 2   ┆ events ┆ 2         ┆ 3           ┆ … ┆ 07:00:00    ┆ null        ┆ 2025-08-17 ┆ 2025-08-17 │\n",
       "│     ┆        ┆           ┆             ┆   ┆             ┆             ┆ 23:45:28.5 ┆ 23:45:28.5 │\n",
       "│     ┆        ┆           ┆             ┆   ┆             ┆             ┆ 68608      ┆ 68608      │\n",
       "│ 307 ┆ events ┆ 306       ┆ 4           ┆ … ┆ 13:00:00    ┆ {\"league_id ┆ 2025-08-17 ┆ 2025-08-17 │\n",
       "│     ┆        ┆           ┆             ┆   ┆             ┆ \":3,\"has_mo ┆ 23:45:28.5 ┆ 23:45:28.5 │\n",
       "│     ┆        ┆           ┆             ┆   ┆             ┆ rning_ev…   ┆ 68608      ┆ 68608      │\n",
       "│ 308 ┆ events ┆ 306       ┆ 5           ┆ … ┆ 13:00:00    ┆ {\"league_id ┆ 2025-08-17 ┆ 2025-08-17 │\n",
       "│     ┆        ┆           ┆             ┆   ┆             ┆ \":3,\"has_mo ┆ 23:45:28.5 ┆ 23:45:28.5 │\n",
       "│     ┆        ┆           ┆             ┆   ┆             ┆ rning_ev…   ┆ 68608      ┆ 68608      │\n",
       "└─────┴────────┴───────────┴─────────────┴───┴─────────────┴─────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5798b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 23:45:51 [INFO] dash_extract - Writing Parquet to s3://allstars-dl-us-west-2-dev/catalog/all_events/ingest_date=2025-08-17/all_events.parquet\n",
      "2025-08-17 23:45:51 [INFO] dash_extract - pg_url=postgresql://appuser:changeme@localhost:5432/analytics_dev\n",
      "2025-08-17 23:45:51 [INFO] dash_extract - Replaced Postgres table events\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Write Parquet to S3 using Polars\n",
    "parquet_key = f\"catalog/{s3_target}/ingest_date={run_date}/{s3_target}.parquet\"\n",
    "\n",
    "log.info(f\"Writing Parquet to s3://{s3_bucket}/{parquet_key}\")\n",
    "write_parquet_to_s3_polars(\n",
    "    df, s3_bucket, parquet_key, compression=\"snappy\", s3_client=s3_client\n",
    ")\n",
    "\n",
    "# 5) Replace table in Postgres\n",
    "load_polars_df_to_postgres(\n",
    "    df, schema=db_schema, table=db_target, conn_str=pg_conn\n",
    ")\n",
    "log.info(f\"Replaced Postgres table {db_target}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "689fc285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 23:55:50 [INFO] dash_extract - pg_url=postgresql://appuser:changeme@localhost:5432/analytics_dev\n",
      "2025-08-17 23:55:50 [INFO] dash_extract - Replaced Postgres table events\n"
     ]
    }
   ],
   "source": [
    "load_polars_df_to_postgres(\n",
    "    df, schema=db_schema, table=db_target, conn_str=pg_conn\n",
    ")\n",
    "log.info(f\"Replaced Postgres table {db_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "process(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vdbt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
